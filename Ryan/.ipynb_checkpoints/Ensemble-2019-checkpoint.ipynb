{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning, using all 8 classifiers individually. After choosing the best parameters for each classifier, they will be input into the ensemble models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shoot_eff</th>\n",
       "      <th>score_op</th>\n",
       "      <th>off_rtg</th>\n",
       "      <th>def_rtg</th>\n",
       "      <th>sos</th>\n",
       "      <th>ie</th>\n",
       "      <th>efg_pct</th>\n",
       "      <th>to_poss</th>\n",
       "      <th>orb_pct</th>\n",
       "      <th>ft_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>seed_7</th>\n",
       "      <th>seed_8</th>\n",
       "      <th>seed_9</th>\n",
       "      <th>seed_10</th>\n",
       "      <th>seed_11</th>\n",
       "      <th>seed_12</th>\n",
       "      <th>seed_13</th>\n",
       "      <th>seed_14</th>\n",
       "      <th>seed_15</th>\n",
       "      <th>seed_16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.042697</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>4.825166</td>\n",
       "      <td>-0.119605</td>\n",
       "      <td>4.944771</td>\n",
       "      <td>4.937317</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>-0.012061</td>\n",
       "      <td>-0.004779</td>\n",
       "      <td>0.078381</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.063741</td>\n",
       "      <td>0.024247</td>\n",
       "      <td>-3.877814</td>\n",
       "      <td>-6.583404</td>\n",
       "      <td>2.705590</td>\n",
       "      <td>3.826096</td>\n",
       "      <td>-0.043163</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.047040</td>\n",
       "      <td>0.057881</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>-0.008396</td>\n",
       "      <td>-0.003129</td>\n",
       "      <td>-1.109305</td>\n",
       "      <td>2.826265</td>\n",
       "      <td>-3.935570</td>\n",
       "      <td>-2.850260</td>\n",
       "      <td>-0.002935</td>\n",
       "      <td>-0.008706</td>\n",
       "      <td>-0.018632</td>\n",
       "      <td>0.023888</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>0.082150</td>\n",
       "      <td>-0.033422</td>\n",
       "      <td>4.205352</td>\n",
       "      <td>0.722199</td>\n",
       "      <td>3.483153</td>\n",
       "      <td>4.072094</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.034606</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.035431</td>\n",
       "      <td>-0.023883</td>\n",
       "      <td>1.412309</td>\n",
       "      <td>-8.231935</td>\n",
       "      <td>9.644244</td>\n",
       "      <td>7.415129</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.021062</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>0.060019</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>-0.100712</td>\n",
       "      <td>-0.047679</td>\n",
       "      <td>-15.683546</td>\n",
       "      <td>3.126334</td>\n",
       "      <td>-18.809879</td>\n",
       "      <td>-11.692840</td>\n",
       "      <td>-0.052438</td>\n",
       "      <td>0.044539</td>\n",
       "      <td>-0.027387</td>\n",
       "      <td>-0.019651</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.059897</td>\n",
       "      <td>0.024216</td>\n",
       "      <td>8.557400</td>\n",
       "      <td>3.386964</td>\n",
       "      <td>5.170436</td>\n",
       "      <td>0.541267</td>\n",
       "      <td>0.025946</td>\n",
       "      <td>-0.033519</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>-0.006627</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>-0.033336</td>\n",
       "      <td>0.046971</td>\n",
       "      <td>1.780265</td>\n",
       "      <td>2.373883</td>\n",
       "      <td>-0.593618</td>\n",
       "      <td>-0.198681</td>\n",
       "      <td>-0.033112</td>\n",
       "      <td>-0.022137</td>\n",
       "      <td>0.022843</td>\n",
       "      <td>0.035003</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>-0.028377</td>\n",
       "      <td>-0.029580</td>\n",
       "      <td>-6.710402</td>\n",
       "      <td>-12.654499</td>\n",
       "      <td>5.944097</td>\n",
       "      <td>6.286750</td>\n",
       "      <td>-0.010597</td>\n",
       "      <td>0.041519</td>\n",
       "      <td>0.022357</td>\n",
       "      <td>0.016638</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>-0.042481</td>\n",
       "      <td>0.030632</td>\n",
       "      <td>-0.428146</td>\n",
       "      <td>-3.754671</td>\n",
       "      <td>3.326525</td>\n",
       "      <td>6.005243</td>\n",
       "      <td>-0.022819</td>\n",
       "      <td>0.017275</td>\n",
       "      <td>0.097348</td>\n",
       "      <td>0.066433</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1062 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      shoot_eff  score_op    off_rtg    def_rtg        sos         ie  \\\n",
       "76     0.042697  0.003724   4.825166  -0.119605   4.944771   4.937317   \n",
       "98    -0.063741  0.024247  -3.877814  -6.583404   2.705590   3.826096   \n",
       "473   -0.008396 -0.003129  -1.109305   2.826265  -3.935570  -2.850260   \n",
       "554    0.082150 -0.033422   4.205352   0.722199   3.483153   4.072094   \n",
       "196    0.035431 -0.023883   1.412309  -8.231935   9.644244   7.415129   \n",
       "...         ...       ...        ...        ...        ...        ...   \n",
       "935   -0.100712 -0.047679 -15.683546   3.126334 -18.809879 -11.692840   \n",
       "247    0.059897  0.024216   8.557400   3.386964   5.170436   0.541267   \n",
       "1242  -0.033336  0.046971   1.780265   2.373883  -0.593618  -0.198681   \n",
       "1274  -0.028377 -0.029580  -6.710402 -12.654499   5.944097   6.286750   \n",
       "428   -0.042481  0.030632  -0.428146  -3.754671   3.326525   6.005243   \n",
       "\n",
       "       efg_pct   to_poss   orb_pct   ft_rate  ...  seed_7  seed_8  seed_9  \\\n",
       "76    0.010645 -0.012061 -0.004779  0.078381  ...       0       0       0   \n",
       "98   -0.043163  0.003544  0.047040  0.057881  ...       0       0       0   \n",
       "473  -0.002935 -0.008706 -0.018632  0.023888  ...       0     255       1   \n",
       "554   0.049600  0.010583 -0.004593 -0.034606  ...     255       0       0   \n",
       "196   0.007332  0.021062  0.005069  0.060019  ...       0       0       0   \n",
       "...        ...       ...       ...       ...  ...     ...     ...     ...   \n",
       "935  -0.052438  0.044539 -0.027387 -0.019651  ...       0       0       0   \n",
       "247   0.025946 -0.033519  0.001564 -0.006627  ...       0       0       0   \n",
       "1242 -0.033112 -0.022137  0.022843  0.035003  ...       0       0       0   \n",
       "1274 -0.010597  0.041519  0.022357  0.016638  ...       0       0       0   \n",
       "428  -0.022819  0.017275  0.097348  0.066433  ...       0       0       0   \n",
       "\n",
       "      seed_10  seed_11  seed_12  seed_13  seed_14  seed_15  seed_16  \n",
       "76          0        0        0        0        0        0        0  \n",
       "98          0        1        0        0        0        0        0  \n",
       "473         0        0        0        0        0        0        0  \n",
       "554         1        0        0        0        0        0        0  \n",
       "196         0        0        0        0        0        0        0  \n",
       "...       ...      ...      ...      ...      ...      ...      ...  \n",
       "935         0        0        0        0        0        0        1  \n",
       "247         0        0        0        0        0        0        0  \n",
       "1242        0        1        0        0        0        0        0  \n",
       "1274        0        0        1        0        0        0        0  \n",
       "428         0        0      255        0        0        0        0  \n",
       "\n",
       "[1062 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For model testing purposes, faster runtime for 455 row x 5 columns\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, log_loss\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "\n",
    "\n",
    "# iris = datasets.load_breast_cancer()\n",
    "# X = pd.DataFrame(iris.data[:, :5])  # we only take the first five features.\n",
    "# y = pd.DataFrame(iris.target)\n",
    "df_model = pd.DataFrame(pd.read_csv('../2019Data/df_Models2019.csv'))\n",
    "X = df_model.iloc[:, :-1]\n",
    "y = df_model.result\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(y_train.unique()) # binary classfication\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(X_train, clf, clf_label='clf'):\n",
    "    \"\"\"\n",
    "    https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n",
    "    Returns an sklearn model pipeline.\n",
    "    \"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      (clf_label, clf)])\n",
    "    return pipe\n",
    "    \n",
    "def clfy_report(clf, X_train, X_test, y_train, y_test, param_grid, clf_label='clf', cv=10):\n",
    "    \"\"\"\n",
    "    Tune classifier hyperparameters and print metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create pipeline steps for encoding categorical variables, feature scaling and normalization \n",
    "    pipe = create_pipe(X_train, clf, clf_label)\n",
    "    \n",
    "    # Instantiate grid search using 10-fold cross validation:\n",
    "    search = RandomizedSearchCV(pipe, param_grid, cv=cv, n_iter=10)\n",
    "    \n",
    "    # Learn relationship between predictors (basketball/tourney features) and outcome,\n",
    "    # and the best parameters for defining such:\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on the test set, new data that haven't been introduced to the model:\n",
    "    predicted = search.predict(X_test)\n",
    "    \n",
    "    # Predictions as probabilities:\n",
    "    probabilities = search.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Accuracy scores for the training and test sets:\n",
    "    train_accuracy = search.score(X_train, y_train)\n",
    "    test_accuracy = search.score(X_test, y_test)\n",
    "\n",
    "    print('Best Parameters: {}\\n'.format(search.best_params_))\n",
    "    print('Training Accuracy: {:0.2}'.format(train_accuracy))\n",
    "    print('Test Accuracy: {:0.2}\\n'.format(test_accuracy))\n",
    "    \n",
    "    # Confusion matrix labels:\n",
    "    labels = np.array([['true losses','false wins'], ['false losses','true wins']])\n",
    "    \n",
    "    # Model evaluation metrics:\n",
    "    confusion_mtrx = confusion_matrix(y_test, predicted)\n",
    "    auc = roc_auc_score(y_test, probabilities)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "    logloss = log_loss(y_test, search.predict_proba(X_test))\n",
    "    \n",
    "    # Plot all metrics in a grid of subplots:\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    grid = plt.GridSpec(2, 4, wspace=0.75, hspace=0.5)\n",
    "    \n",
    "    # Top-left plot - confusion matrix:\n",
    "    plt.subplot(grid[0, :2])\n",
    "    sns.heatmap(confusion_mtrx, annot=True, fmt=\"d\") #, fmt='')\n",
    "    plt.xlabel('Predicted Games')\n",
    "    plt.ylabel('Actual Games');\n",
    "    \n",
    "    # Top-right plot - ROC curve:\n",
    "    plt.subplot(grid[0, 2:])\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('AUROC: {:0.3}'.format(auc));\n",
    "    \n",
    "    # Bottom-left plot - support, or true predictions:\n",
    "    plt.subplot(grid[1, :2])\n",
    "    sns.countplot(y=predicted, orient='h')\n",
    "    plt.yticks([1, 0], ('wins', 'losses'))\n",
    "    plt.ylabel(''), plt.xlabel('Number Predicted');\n",
    "    \n",
    "    # Bottom-right plot - classification report:\n",
    "    plt.subplot(grid[1, 2:])\n",
    "    visualizer = ClassificationReport(search, classes=['losses', 'wins'])\n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_test, y_test)\n",
    "    g = visualizer.poof();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    SVC(probability=True),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    MLPClassifier(max_iter=500), # to allow gaurenteed convergence. \n",
    "    GaussianNB(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    XGBClassifier()\n",
    "    ]\n",
    "\n",
    "\n",
    "# clf__kernel\n",
    "params=[\n",
    "    { # SVC\n",
    "        'clf__kernel': ['rbf', 'linear', 'sigmoid'],\n",
    "        'clf__C': np.logspace(start=-10, stop=10, num=21) # default 1.0\n",
    "    },\n",
    "    { # RandomForestClassifier\n",
    "        'clf__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'clf__max_features': ['sqrt', 'log2'] # The lower the greater the reduction of variance, but also the greater the increase in bias.\n",
    "    },\n",
    "    { # LogisticRegression\n",
    "        'clf__C': np.logspace(start=-10, stop=10, num=21),\n",
    "        'clf__penalty': ['none', 'l2']\n",
    "    },\n",
    "    { # Neural network multi-layered perceptron, MLPClassifier\n",
    "      'clf__hidden_layer_sizes': tuple(map(tuple, np.random.randint(low=5, high=20, size=(10, 3)))) # from 5-20 nodes per 3 layers, 10 iterations\n",
    "    },\n",
    "    { # GaussianNB\n",
    "        'clf__var_smoothing': [1e-8, 1e-9, 1e-10]\n",
    "    },\n",
    "    { # AdaBoostClassifier\n",
    "        'clf__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'clf__learning_rate': np.linspace(0.5, 1.5, 10, endpoint=True) # default 1.0\n",
    "    },\n",
    "    { # GradientBoostingClassifier\n",
    "        'clf__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'clf__learning_rate': np.linspace(0.02, 0.18, 9, endpoint=True) # default 0.1\n",
    "    },\n",
    "    { # XGBClassifier\n",
    "#     'clf__learning_rate': np.logspace(start=0.01, stop=0.2, num=10, endpoint = True), # see last example in np.logspace documentation\n",
    "    'clf__max_depth': [2,3,4,5],\n",
    "    'clf__booster': ['gbtree', 'gblinear', 'dart']\n",
    "    }\n",
    "]\n",
    "\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "auc=[]\n",
    "loggloss=[]\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    print(classifier)\n",
    "    clfy_report(classifier, X_train, X_test, y_train, y_test, param_grid=params[i], cv=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods: VotingClassifier, StackingClassifier. \n",
    "## 2019 uses same method as 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: High bias implies an underfit model, high variance implies an overfit model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, a bagging method using a VotingClassifier will be used to reduce the amount of variance in the model. Then, a boosting method using a StackingClassifier will be run comparatively to reduce bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ensemble averaging methods (VotingClassifier): average predictions to reduce variance. \n",
    "- ensemble boosting methods (StackingClassifier): base estimators are built sequentially and one tries to reduce the bias of the combined estimator.\n",
    "- Bagging methods work best with strong and complex models (e.g. where the data points are unpredictable), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).\n",
    "- Model with high bias pays very little attention to the training data and oversimplifies the model.\n",
    "- High variance models pays attention to training data and does not generalize on the data which it hasn’t seen before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe_ensemble(X_train, ensemble_clf, clf_label='clf'):\n",
    "    \"\"\"\n",
    "    https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n",
    "    Returns an sklearn model pipeline.\n",
    "    \"\"\"\n",
    "    pipe = Pipeline([(clf_label, VotingClassifier(estimators=[\n",
    "                          (clf_label+str(1), create_pipe(X_train, ensemble_clf.estimators[0][1])),\n",
    "                          (clf_label+str(2), create_pipe(X_train, ensemble_clf.estimators[1][1])),\n",
    "                          (clf_label+str(3), create_pipe(X_train, ensemble_clf.estimators[2][1])),\n",
    "                          (clf_label+str(4), create_pipe(X_train, ensemble_clf.estimators[3][1])),\n",
    "                          (clf_label+str(5), create_pipe(X_train, ensemble_clf.estimators[4][1])),\n",
    "                          (clf_label+str(6), create_pipe(X_train, ensemble_clf.estimators[5][1])),\n",
    "                          (clf_label+str(7), create_pipe(X_train, ensemble_clf.estimators[6][1])),\n",
    "                          (clf_label+str(8), create_pipe(X_train, ensemble_clf.estimators[7][1]))\n",
    "                                           ]))])\n",
    "    return pipe\n",
    "\n",
    "def clfy_report_ensemble(ensemble_voting, X_train, X_test, y_train, y_test, ensemble_params, cv=10):\n",
    "    \"\"\"\n",
    "    Tune classifier hyperparameters and print metrics.\n",
    "    return search, train_accuracy, test_accuracy, auc, logloss\n",
    "    \"\"\"\n",
    "    # Instantiate grid search using 10-fold cross validation:\n",
    "    search = RandomizedSearchCV(ensemble_voting, ensemble_params, cv=cv, n_iter=5)\n",
    "    \n",
    "    # Learn relationship between predictors (basketball/tourney features) and outcome,\n",
    "    # and the best parameters for defining such:\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on the test set, new data that haven't been introduced to the model:\n",
    "    predicted = search.predict(X_test)\n",
    "    \n",
    "    # Predictions as probabilities:\n",
    "    probabilities = search.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Accuracy scores for the training and test sets:\n",
    "    train_accuracy = search.score(X_train, y_train)\n",
    "    test_accuracy = search.score(X_test, y_test)\n",
    "\n",
    "    print('Best Parameters: {}\\n'.format(search.best_params_))\n",
    "    print('Training Accuracy: {:0.2}'.format(train_accuracy))\n",
    "    print('Test Accuracy: {:0.2}\\n'.format(test_accuracy))\n",
    "    \n",
    "    # Confusion matrix labels:\n",
    "    labels = np.array([['true losses','false wins'], ['false losses','true wins']])\n",
    "    \n",
    "    # Model evaluation metrics:\n",
    "    confusion_mtrx = confusion_matrix(y_test, predicted)\n",
    "    auc = roc_auc_score(y_test, probabilities)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "    logloss = log_loss(y_test, search.predict_proba(X_test))\n",
    "    \n",
    "    # Plot all metrics in a grid of subplots:\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    grid = plt.GridSpec(2, 4, wspace=0.75, hspace=0.5)\n",
    "    \n",
    "    # Top-left plot - confusion matrix:\n",
    "    plt.subplot(grid[0, :2])\n",
    "    sns.heatmap(confusion_mtrx, annot=True, fmt=\"d\") #, fmt='')\n",
    "    plt.xlabel('Predicted Games')\n",
    "    plt.ylabel('Actual Games');\n",
    "    \n",
    "    # Top-right plot - ROC curve:\n",
    "    plt.subplot(grid[0, 2:])\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('AUROC: {:0.3}'.format(auc));\n",
    "    \n",
    "    # Bottom-left plot - support, or true predictions:\n",
    "    plt.subplot(grid[1, :2])\n",
    "    sns.countplot(y=predicted, orient='h')\n",
    "    plt.yticks([1, 0], ('wins', 'losses'))\n",
    "    plt.ylabel(''), plt.xlabel('Number Predicted');\n",
    "    \n",
    "    # Bottom-right plot - classification report:\n",
    "    plt.subplot(grid[1, 2:])\n",
    "    visualizer = ClassificationReport(search, classes=['losses', 'wins'])\n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_test, y_test)\n",
    "    g = visualizer.poof();\n",
    "    \n",
    "    return search, train_accuracy, test_accuracy, auc, logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# From hyperparameter tuning section above:\n",
    "# Best Parameters: {'clf__kernel': 'rbf', 'clf__C': 1.0}\n",
    "# Best Parameters: {'clf__n_estimators': 144, 'clf__max_features': 'log2'}\n",
    "# Best Parameters: {'clf__penalty': 'l2', 'clf__C': 10000.0}\n",
    "# Best Parameters: {'clf__hidden_layer_sizes': (5, 18, 8)}\n",
    "# Best Parameters: {'clf__var_smoothing': 1e-08}\n",
    "# Best Parameters: {'clf__n_estimators': 177, 'clf__learning_rate': 0.8333333333333333}\n",
    "# Best Parameters: {'clf__n_estimators': 144, 'clf__learning_rate': 0.04}\n",
    "# Best Parameters: {'clf__max_depth': 4, 'clf__booster': 'gblinear'}\n",
    "classifiers = [\n",
    "    SVC(probability=True, kernel='rbf', C=1.0),         \n",
    "    RandomForestClassifier(n_estimators=144, max_features='log2'),      \n",
    "    LogisticRegression(penalty='l2', C=10000.0),          \n",
    "    MLPClassifier(max_iter=500, hidden_layer_sizes=(5, 18, 8)),   \n",
    "    GaussianNB(var_smoothing=1e-08),                  \n",
    "    AdaBoostClassifier(n_estimators=177, learning_rate=0.83),          \n",
    "    GradientBoostingClassifier(n_estimators=144, learning_rate=0.04), \n",
    "    XGBClassifier(max_depth=4, booster='gblinear')             \n",
    "    ]\n",
    "\n",
    "pipeline_classifiers=[]\n",
    "for i in range(0,len(classifiers)):\n",
    "    pipeline_classifiers.append(Pipeline (steps=[('preprocessor', preprocessor),\n",
    "        ('clf'+str(i+1), classifiers[i])]))\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble_voting = Pipeline(steps=[('ensemble', VotingClassifier(estimators=[\n",
    "    ('svc', pipeline_classifiers[0]), \n",
    "    ('rdf' , pipeline_classifiers[1]),\n",
    "    ('lgr' , pipeline_classifiers[2]),\n",
    "    ('mlp', pipeline_classifiers[3]),\n",
    "    ('gau', pipeline_classifiers[4]),\n",
    "    ('ada', pipeline_classifiers[5]), \n",
    "    ('gbt', pipeline_classifiers[6]),\n",
    "    ('xgb', pipeline_classifiers[7])], \n",
    "                                voting='soft', \n",
    "                                # weights = [1,2,3], \n",
    "                                n_jobs=-1))])\n",
    "\n",
    "import pprint as pp\n",
    "pp.pprint(sorted(ensemble_voting.get_params().keys())) # used to specify ensemble params, below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_params=[\n",
    "    { # SVC\n",
    "        'ensemble__svc__clf1__kernel': ['rbf', 'linear', 'sigmoid'],\n",
    "        'ensemble__svc__clf1__C': np.logspace(start=-10, stop=10, num=21) # default 1.0\n",
    "    },\n",
    "    { # RandomForestClassifier\n",
    "        'ensemble__rdf__clf2__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'ensemble__rdf__clf2__max_features': ['sqrt', 'log2'] # The lower the greater the reduction of variance, but also the greater the increase in bias.\n",
    "    },\n",
    "    { # LogisticRegression\n",
    "        'ensemble__lgr__clf3__C': np.logspace(start=-10, stop=10, num=21),\n",
    "        'ensemble__lgr__clf3__penalty': ['none', 'l2']\n",
    "    },\n",
    "    { # Neural network multi-layered perceptron, MLPClassifier\n",
    "      'ensemble__mlp__clf4__hidden_layer_sizes': tuple(map(tuple, np.random.randint(low=5, high=20, size=(10, 3)))) # from 5-20 nodes per 3 layers, 10 iterations\n",
    "    },\n",
    "    { # GaussianNB\n",
    "        'ensemble__gau__clf5__var_smoothing': [1e-8, 1e-9, 1e-10]\n",
    "    },\n",
    "    { # AdaBoostClassifier\n",
    "        'ensemble__ada__clf6__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'ensemble__ada__clf6__learning_rate': np.linspace(0.5, 1.5, 10, endpoint=True) # default 1.0\n",
    "    },\n",
    "    { # GradientBoostingClassifier\n",
    "        'ensemble__gbt__clf7__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'ensemble__gbt__clf7__learning_rate': np.linspace(0.02, 0.18, 9, endpoint=True) # default 0.1\n",
    "    },\n",
    "    { # XGBClassifier\n",
    "#     'clf__learning_rate': np.logspace(start=0.01, stop=0.2, num=10, endpoint = True), # see last example in np.logspace documentation\n",
    "    'ensemble__xgb__clf8__max_depth': [2,3,4,5],\n",
    "    'ensemble__xgb__clf8__booster': ['gbtree', 'gblinear', 'dart']\n",
    "    }\n",
    "]\n",
    "timer_start=time.time()\n",
    "clfy_report_ensemble(ensemble_voting, X_train, X_test, y_train, y_test, ensemble_params, cv=10)\n",
    "print(search_ens, train_accuracy, test_accuracy, auc, logloss)\n",
    "print(\"Time taken:\", round(-(timer_start-time.time()),2),'seconds. ')\n",
    "del timer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('svc',  SVC(probability=True, kernel='rbf', C=1.0         )),\n",
    "    ('rdf' , RandomForestClassifier(n_estimators=144, max_features='log2'      )),\n",
    "    ('lgr' , LogisticRegression(penalty='l2', C=10000.0          )),\n",
    "    ('mlp',  MLPClassifier(max_iter=500, hidden_layer_sizes=(5, 18, 8)   )),\n",
    "    ('gau',  GaussianNB(var_smoothing=1e-08)),                  \n",
    "    ('ada',  AdaBoostClassifier(n_estimators=177, learning_rate=0.83          )),\n",
    "    ('gbt',  GradientBoostingClassifier(n_estimators=144, learning_rate=0.04) ),\n",
    "    ('xgb',  XGBClassifier(max_depth=4, booster='gblinear')                      )\n",
    "    ]\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "stack_clf = StackingClassifier(estimators=classifiers, final_estimator=LogisticRegression())\n",
    "pipe_stack_clf = Pipeline(steps=[('preproc', preprocessor), ('stack', stack_clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(sorted(pipe_stack_clf.get_params().keys())) # used to specify pipe_stack_clf params, below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stack_clf_params=[\n",
    "    { # SVC\n",
    "        'stack__svc__kernel': ['rbf', 'linear', 'sigmoid'],\n",
    "        'stack__svc__C': np.logspace(start=-10, stop=10, num=21) # default 1.0\n",
    "    },\n",
    "    { # RandomForestClassifier\n",
    "        'stack__rdf__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'stack__rdf__max_features': ['sqrt', 'log2'] # The lower the greater the reduction of variance, but also the greater the increase in bias.\n",
    "    },\n",
    "    { # LogisticRegression\n",
    "        'stack__lgr__C': np.logspace(start=-10, stop=10, num=21),\n",
    "        'stack__lgr__penalty': ['none', 'l2']\n",
    "    },\n",
    "    { # Neural network multi-layered perceptron, MLPClassifier\n",
    "        'stack__mlp__hidden_layer_sizes': tuple(map(tuple, np.random.randint(low=5, high=20, size=(10, 3)))) # from 5-20 nodes per 3 layers, 10 iterations\n",
    "    },\n",
    "    { # GaussianNB\n",
    "        'stack__gau__var_smoothing': [1e-8, 1e-9, 1e-10]\n",
    "    },\n",
    "    { # AdaBoostClassifier\n",
    "        'stack__ada__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'stack__ada__learning_rate': np.linspace(0.5, 1.5, 10, endpoint=True) # default 1.0\n",
    "    },\n",
    "    { # GradientBoostingClassifier\n",
    "        'stack__gbt__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'stack__gbt__learning_rate': np.linspace(0.02, 0.18, 9, endpoint=True) # default 0.1\n",
    "    },\n",
    "    { # XGBClassifier\n",
    "#     'clf__learning_rate': np.logspace(start=0.01, stop=0.2, num=10, endpoint = True), # see last example in np.logspace documentation\n",
    "        'stack__xgb__max_depth': [2,3,4,5],\n",
    "        'stack__xgb__booster': ['gbtree', 'gblinear', 'dart']\n",
    "    }\n",
    "]\n",
    "# print(search, train_accuracy, test_accuracy, auc, logloss)\n",
    "timer_start=time.time()\n",
    "search_stk, train_accuracy, test_accuracy, auc, logloss = clfy_report_ensemble(pipe_stack_clf, X_train, X_test, y_train, y_test, pipe_stack_clf_params, cv=10)\n",
    "print(search_stk, train_accuracy, test_accuracy, auc, logloss)\n",
    "print(\"Time taken:\", round(-(timer_start-time.time()),2),'seconds. ')\n",
    "del timer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackedClassifier loss, bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_start=time.time()\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        stack_clf, np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), \n",
    "        loss='0-1_loss')\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print(\"Time taken:\", round(-(timer_start-time.time()),2),'seconds. ')\n",
    "del timer_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VotingClassifier loss, bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_start=time.time()\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "#         ensemble_voting[0], np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), \n",
    "    VotingClassifier(estimators=[\n",
    "        classifiers[0], \n",
    "        classifiers[1],\n",
    "        classifiers[2],\n",
    "        classifiers[3],\n",
    "        classifiers[4],\n",
    "        classifiers[5], \n",
    "        classifiers[6],\n",
    "        classifiers[7]], \n",
    "                        voting='soft', \n",
    "                        # weights = [1,2,3], \n",
    "                        n_jobs=-1),\n",
    "        np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), \n",
    "        loss='0-1_loss')\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print(\"Time taken:\", round(-(timer_start-time.time()),2),'seconds. ')\n",
    "del timer_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ensemble averaging methods (VotingClassifier): average predictions to reduce variance. \n",
    "- ensemble boosting methods (StackingClassifier): base estimators are built sequentially and one tries to reduce the bias of the combined estimator.\n",
    "- Bagging methods work best with strong and complex models (e.g. where the data points are unpredictable), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).\n",
    "- Model with high bias pays very little attention to the training data and oversimplifies the model.\n",
    "- High variance models pays attention to training data and does not generalize on the data which it hasn’t seen before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From these results, the both of the VotingClassifier and StackingClassifier models built have:\n",
    "- very low variance  (~0.10 - 0.14)\n",
    "- low-moderate loss (~0.31 - 0.34)\n",
    "- low-moderate bias (~0.30 - 0.32)  </ul>\n",
    "The results above implies that the final model needs to use the StackingClassifier, to lower bias. However, the 10-fold cross-validated accuracy is around the same for the VotingClassifier (Training Accuracy: 0.82, Test Accuracy: 0.69) compared to the StackingClassifier (Training Accuracy: 0.64, Test Accuracy: 0.7). \n",
    "### Therefore, either VotingClassifier or StackingClassifier could be used as the primary prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a bracket using both methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.read_csv('../2019Data/SampleSubmissionStage2.csv')\n",
    "df_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_team1_team2(ID):\n",
    "    \"\"\"Return a tuple with the year, team1 and team2\n",
    "    for each ID in the sample submission file of possible matches.\"\"\"\n",
    "    return (int(x) for x in ID.split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv('../2019Data/df_features.csv')\n",
    "diff = []\n",
    "data = []\n",
    "\n",
    "for i, row in df_predict.iterrows():\n",
    "\n",
    "    year, team1, team2 = get_year_team1_team2(row.ID)\n",
    "\n",
    "    # Save 2018 stats/features for the first ID:\n",
    "    team1 = df_features[(df_features['Season'] == year) & (df_features['TeamID'] == team1)].values[0]\n",
    "\n",
    "    # Save 2018 stats/features for the first ID:\n",
    "    team2 = df_features[(df_features['Season'] == year) & (df_features['TeamID'] == team2)].values[0]\n",
    "\n",
    "    diff = team1 - team2\n",
    "\n",
    "    data.append(diff)\n",
    "\n",
    "n_poss_games = len(df_predict)\n",
    "columns = df_features.columns.get_values()\n",
    "final_predictions = pd.DataFrame(np.array(data).reshape(n_poss_games, np.array(data).shape[1]), columns=(columns))\n",
    "final_predictions.drop(['Season', 'TeamID'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackingClassifier Bracket Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = search_stk.predict_proba(final_predictions)[:, 1]\n",
    "clipped_predictions = np.clip(predictions, 0.05, 0.95)\n",
    "df_predict.Pred = clipped_predictions\n",
    "df_predict.to_csv('search_stk.csv', index=False)\n",
    "\n",
    "from bracketeer import build_bracket\n",
    "b = build_bracket(\n",
    "        outputPath='search_stk.png', # in /Ryan\n",
    "        submissionPath='search_stk.csv',\n",
    "        teamsPath='../2020Data/MDataFiles_Stage1/MTeams.csv',\n",
    "        seedsPath='../2020Data/MDataFiles_Stage1/MNCAATourneySeeds.csv',\n",
    "        slotsPath='../2020Data/MDataFiles_Stage1/MNCAATourneySlots.csv',\n",
    "        year=2019\n",
    ")\n",
    "from IPython.display import Image\n",
    "Image(filename='search_stk.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier Bracket Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = search_ens.predict_proba(final_predictions)[:, 1]\n",
    "clipped_predictions = np.clip(predictions, 0.05, 0.95)\n",
    "df_predict.Pred = clipped_predictions\n",
    "df_predict.to_csv('search_ens.csv', index=False)\n",
    "\n",
    "from bracketeer import build_bracket\n",
    "b = build_bracket(\n",
    "        outputPath='search_ens.png', # in /Ryan\n",
    "        submissionPath='search_ens.csv',\n",
    "        teamsPath='../2020Data/MDataFiles_Stage1/MTeams.csv',\n",
    "        seedsPath='../2020Data/MDataFiles_Stage1/MNCAATourneySeeds.csv',\n",
    "        slotsPath='../2020Data/MDataFiles_Stage1/MNCAATourneySlots.csv',\n",
    "        year=2019\n",
    ")\n",
    "from IPython.display import Image\n",
    "Image(filename='search_ens.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
