{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning, using all 8 classifiers individually. After choosing the best parameters for each classifier, they will be input into the ensemble models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shoot_eff</th>\n",
       "      <th>score_op</th>\n",
       "      <th>off_rtg</th>\n",
       "      <th>def_rtg</th>\n",
       "      <th>sos</th>\n",
       "      <th>ie</th>\n",
       "      <th>efg_pct</th>\n",
       "      <th>to_poss</th>\n",
       "      <th>orb_pct</th>\n",
       "      <th>ft_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>seed_7</th>\n",
       "      <th>seed_8</th>\n",
       "      <th>seed_9</th>\n",
       "      <th>seed_10</th>\n",
       "      <th>seed_11</th>\n",
       "      <th>seed_12</th>\n",
       "      <th>seed_13</th>\n",
       "      <th>seed_14</th>\n",
       "      <th>seed_15</th>\n",
       "      <th>seed_16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.010383</td>\n",
       "      <td>0.010420</td>\n",
       "      <td>2.065874</td>\n",
       "      <td>1.369455</td>\n",
       "      <td>0.696419</td>\n",
       "      <td>1.379193</td>\n",
       "      <td>0.005367</td>\n",
       "      <td>-0.002182</td>\n",
       "      <td>0.024726</td>\n",
       "      <td>0.023743</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>-0.018557</td>\n",
       "      <td>0.018141</td>\n",
       "      <td>0.665276</td>\n",
       "      <td>0.421097</td>\n",
       "      <td>0.244179</td>\n",
       "      <td>1.796148</td>\n",
       "      <td>-0.012351</td>\n",
       "      <td>-0.022971</td>\n",
       "      <td>-0.008802</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>0.105896</td>\n",
       "      <td>-0.008048</td>\n",
       "      <td>9.763181</td>\n",
       "      <td>4.680730</td>\n",
       "      <td>5.082451</td>\n",
       "      <td>2.546429</td>\n",
       "      <td>0.060811</td>\n",
       "      <td>-0.021138</td>\n",
       "      <td>-0.027615</td>\n",
       "      <td>-0.009111</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.126395</td>\n",
       "      <td>-0.000838</td>\n",
       "      <td>12.185547</td>\n",
       "      <td>3.469630</td>\n",
       "      <td>8.715917</td>\n",
       "      <td>4.780036</td>\n",
       "      <td>0.058712</td>\n",
       "      <td>-0.019422</td>\n",
       "      <td>-0.012708</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.005848</td>\n",
       "      <td>-0.019555</td>\n",
       "      <td>-1.247879</td>\n",
       "      <td>7.960186</td>\n",
       "      <td>-9.208065</td>\n",
       "      <td>-7.418771</td>\n",
       "      <td>0.006311</td>\n",
       "      <td>0.012156</td>\n",
       "      <td>-0.007190</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.055898</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>6.811015</td>\n",
       "      <td>-0.778397</td>\n",
       "      <td>7.589412</td>\n",
       "      <td>2.528593</td>\n",
       "      <td>0.021272</td>\n",
       "      <td>-0.014404</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.033474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>0.009229</td>\n",
       "      <td>-0.021085</td>\n",
       "      <td>-1.639563</td>\n",
       "      <td>-1.936270</td>\n",
       "      <td>0.296706</td>\n",
       "      <td>-0.546009</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>-0.012593</td>\n",
       "      <td>-0.069562</td>\n",
       "      <td>-0.042920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.054235</td>\n",
       "      <td>6.836000</td>\n",
       "      <td>-6.515616</td>\n",
       "      <td>13.351616</td>\n",
       "      <td>8.553518</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>0.108363</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>-0.124139</td>\n",
       "      <td>-0.034062</td>\n",
       "      <td>-15.593936</td>\n",
       "      <td>-6.490539</td>\n",
       "      <td>-9.103397</td>\n",
       "      <td>-0.273377</td>\n",
       "      <td>-0.071892</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.076976</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>0.078895</td>\n",
       "      <td>-0.054045</td>\n",
       "      <td>1.194558</td>\n",
       "      <td>2.926029</td>\n",
       "      <td>-1.731470</td>\n",
       "      <td>-0.796967</td>\n",
       "      <td>0.047655</td>\n",
       "      <td>0.020214</td>\n",
       "      <td>-0.035365</td>\n",
       "      <td>0.022679</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1062 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      shoot_eff  score_op    off_rtg   def_rtg        sos        ie   efg_pct  \\\n",
       "395    0.010383  0.010420   2.065874  1.369455   0.696419  1.379193  0.005367   \n",
       "549   -0.018557  0.018141   0.665276  0.421097   0.244179  1.796148 -0.012351   \n",
       "841    0.105896 -0.008048   9.763181  4.680730   5.082451  2.546429  0.060811   \n",
       "108    0.126395 -0.000838  12.185547  3.469630   8.715917  4.780036  0.058712   \n",
       "80     0.005848 -0.019555  -1.247879  7.960186  -9.208065 -7.418771  0.006311   \n",
       "...         ...       ...        ...       ...        ...       ...       ...   \n",
       "126    0.055898  0.012231   6.811015 -0.778397   7.589412  2.528593  0.021272   \n",
       "947    0.009229 -0.021085  -1.639563 -1.936270   0.296706 -0.546009  0.011564   \n",
       "465    0.000643  0.054235   6.836000 -6.515616  13.351616  8.553518  0.001903   \n",
       "829   -0.124139 -0.034062 -15.593936 -6.490539  -9.103397 -0.273377 -0.071892   \n",
       "1090   0.078895 -0.054045   1.194558  2.926029  -1.731470 -0.796967  0.047655   \n",
       "\n",
       "       to_poss   orb_pct   ft_rate  ...  seed_7  seed_8  seed_9  seed_10  \\\n",
       "395  -0.002182  0.024726  0.023743  ...       0       0     255        0   \n",
       "549  -0.022971 -0.008802  0.000361  ...       0       0       0        0   \n",
       "841  -0.021138 -0.027615 -0.009111  ...       0       0       0        0   \n",
       "108  -0.019422 -0.012708  0.017715  ...       0       0       0        0   \n",
       "80    0.012156 -0.007190 -0.002546  ...       0       0       0        0   \n",
       "...        ...       ...       ...  ...     ...     ...     ...      ...   \n",
       "126  -0.014404 -0.001778  0.033474  ...       0       0       0        0   \n",
       "947  -0.012593 -0.069562 -0.042920  ...       0       0       0        0   \n",
       "465  -0.000857  0.108363  0.015492  ...       0       0       0        0   \n",
       "829   0.042400  0.018895  0.076976  ...       0       0       0        0   \n",
       "1090  0.020214 -0.035365  0.022679  ...       0       0       0        0   \n",
       "\n",
       "      seed_11  seed_12  seed_13  seed_14  seed_15  seed_16  \n",
       "395         0        0        0        0        0        0  \n",
       "549         0        0        0      255        0        0  \n",
       "841         0        0        0        0        0        0  \n",
       "108         0        0        0        0      255        0  \n",
       "80          1        0        0        0        0        0  \n",
       "...       ...      ...      ...      ...      ...      ...  \n",
       "126         0        0        0        0        0        0  \n",
       "947         0        0        0        1        0        0  \n",
       "465         0        0        0        0      255        0  \n",
       "829         0        0        0        0        0        0  \n",
       "1090        0        0        1        0        0        0  \n",
       "\n",
       "[1062 rows x 34 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, log_loss\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df_model = pd.DataFrame(pd.read_csv('../2019Data/df_Models2019.csv'))\n",
    "X = df_model.iloc[:, :-1]\n",
    "y = df_model.result\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(y_train.unique()) # binary classfication\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(X_train, clf, clf_label='clf'):\n",
    "    \"\"\"\n",
    "    https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n",
    "    Returns an sklearn model pipeline.\n",
    "    \"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      (clf_label, clf)])\n",
    "    return pipe\n",
    "    \n",
    "def clfy_report(clf, X_train, X_test, y_train, y_test, param_grid, clf_label='clf', cv=10):\n",
    "    \"\"\"\n",
    "    Tune classifier hyperparameters and print metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create pipeline steps for encoding categorical variables, feature scaling and normalization \n",
    "    pipe = create_pipe(X_train, clf, clf_label)\n",
    "    \n",
    "    # Instantiate grid search using 10-fold cross validation:\n",
    "    # Learn relationship between predictors (basketball/tourney features) and outcome,\n",
    "    # and the best parameters for defining such:\n",
    "    search = RandomizedSearchCV(pipe, param_grid, cv=cv, n_iter=3).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Predictions on the test set, new data that haven't been introduced to the model:\n",
    "    predicted = search.predict(X_test)\n",
    "    \n",
    "    # Predictions as probabilities:\n",
    "    probabilities = search.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Accuracy scores for the training and test sets:\n",
    "    train_accuracy = search.score(X_train, y_train)\n",
    "    test_accuracy = search.score(X_test, y_test)\n",
    "\n",
    "    print('Best Parameters: {}\\n'.format(search.best_params_))\n",
    "    print('Training Accuracy: {:0.2}'.format(train_accuracy))\n",
    "    print('Test Accuracy: {:0.2}\\n'.format(test_accuracy))\n",
    "    \n",
    "    # Confusion matrix labels:\n",
    "    labels = np.array([['true losses','false wins'], ['false losses','true wins']])\n",
    "    \n",
    "    # Model evaluation metrics:\n",
    "    confusion_mtrx = confusion_matrix(y_test, predicted)\n",
    "    auc = roc_auc_score(y_test, probabilities)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "    logloss = log_loss(y_test, search.predict_proba(X_test))\n",
    "    \n",
    "    # Plot all metrics in a grid of subplots:\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    grid = plt.GridSpec(2, 4, wspace=0.75, hspace=0.5)\n",
    "    \n",
    "    # Top-left plot - confusion matrix:\n",
    "    plt.subplot(grid[0, :2])\n",
    "    sns.heatmap(confusion_mtrx, annot=True, fmt=\"d\") #, fmt='')\n",
    "    plt.xlabel('Predicted Games')\n",
    "    plt.ylabel('Actual Games');\n",
    "    \n",
    "    # Top-right plot - ROC curve:\n",
    "    plt.subplot(grid[0, 2:])\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('AUROC: {:0.3}'.format(auc));\n",
    "    \n",
    "    # Bottom-left plot - support, or true predictions:\n",
    "    plt.subplot(grid[1, :2])\n",
    "    sns.countplot(y=predicted, orient='h')\n",
    "    plt.yticks([1, 0], ('wins', 'losses'))\n",
    "    plt.ylabel(''), plt.xlabel('Number Predicted');\n",
    "    \n",
    "    # Bottom-right plot - classification report:\n",
    "    plt.subplot(grid[1, 2:])\n",
    "    visualizer = ClassificationReport(search, classes=['losses', 'wins'])\n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_test, y_test)\n",
    "    g = visualizer.poof();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    SVC(probability=True),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    MLPClassifier(max_iter=500), # to allow gaurenteed convergence. \n",
    "    GaussianNB(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    XGBClassifier()\n",
    "    ]\n",
    "\n",
    "\n",
    "# clf__kernel\n",
    "params=[\n",
    "    { # SVC\n",
    "        'clf__kernel': ['rbf', 'linear', 'sigmoid'],\n",
    "        'clf__C': np.logspace(start=-10, stop=10, num=21) # default 1.0\n",
    "    },\n",
    "    { # RandomForestClassifier\n",
    "        'clf__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'clf__max_features': ['sqrt', 'log2'] # The lower the greater the reduction of variance, but also the greater the increase in bias.\n",
    "    },\n",
    "    { # LogisticRegression\n",
    "        'clf__C': np.logspace(start=-10, stop=10, num=21),\n",
    "        'clf__penalty': ['none', 'l2']\n",
    "    },\n",
    "    { # Neural network multi-layered perceptron, MLPClassifier\n",
    "      'clf__hidden_layer_sizes': tuple(map(tuple, np.random.randint(low=5, high=20, size=(10, 3)))) # from 5-20 nodes per 3 layers, 10 iterations\n",
    "    },\n",
    "    { # GaussianNB\n",
    "        'clf__var_smoothing': [1e-8, 1e-9, 1e-10]\n",
    "    },\n",
    "    { # AdaBoostClassifier\n",
    "        'clf__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'clf__learning_rate': np.linspace(0.5, 1.5, 10, endpoint=True) # default 1.0\n",
    "    },\n",
    "    { # GradientBoostingClassifier\n",
    "        'clf__n_estimators': np.array(np.linspace(100,200,10, endpoint=True) , dtype=np.int32), # default 100\n",
    "        'clf__learning_rate': np.linspace(0.02, 0.18, 9, endpoint=True) # default 0.1\n",
    "    },\n",
    "    { # XGBClassifier\n",
    "#     'clf__learning_rate': np.logspace(start=0.01, stop=0.2, num=10, endpoint = True), # see last example in np.logspace documentation\n",
    "    'clf__max_depth': [2,3,4,5],\n",
    "    'clf__booster': ['gbtree', 'gblinear', 'dart']\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    print(classifier)\n",
    "    clfy_report(classifier, X_train, X_test, y_train, y_test, param_grid=params[i], cv=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods: VotingClassifier, StackingClassifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: High bias implies an underfit model, high variance implies an overfit model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, a bagging method using a VotingClassifier will be used to reduce the amount of variance in the model. Then, a boosting method using a StackingClassifier will be run comparatively to reduce bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ensemble averaging methods (VotingClassifier): average predictions to reduce variance. \n",
    "- ensemble boosting methods (StackingClassifier): base estimators are built sequentially and one tries to reduce the bias of the combined estimator.\n",
    "- Bagging methods work best with strong and complex models (e.g. where the data points are unpredictable), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).\n",
    "- Model with high bias pays very little attention to the training data and oversimplifies the model.\n",
    "- High variance models pays attention to training data and does not generalize on the data which it hasn’t seen before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackingClassifier/VotingClassifier Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clfy_report_ensemble(ensemble_voting, X, y, cv=10):\n",
    "    \"\"\"\n",
    "    Using an ensemble classifier, generate cross validated results. \n",
    "    Using StratifiedKFold to ensure that the classes are balanced equally in both\n",
    "        training and testing, since some splits may be imbalanced. \n",
    "    \"\"\"\n",
    "    count_train_accuracy=0\n",
    "    count_test_accuracy=0\n",
    "    count_auc=0\n",
    "    count_logloss=0\n",
    "    count_fold=0\n",
    "    \n",
    "    skf=StratifiedKFold(n_splits=cv, random_state=None, shuffle=False)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        count_fold+=1\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        ensemble_voting.fit(X_train, y_train)\n",
    "\n",
    "        predicted = ensemble_voting.predict(X_test)\n",
    "\n",
    "        # Predictions as probabilities:\n",
    "        probabilities = ensemble_voting.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Accuracy scores for the training and test sets:\n",
    "        train_accuracy = ensemble_voting.score(X_train, y_train)\n",
    "        test_accuracy = ensemble_voting.score(X_test, y_test)\n",
    "\n",
    "        print('For fold: {}'.format(count_fold))\n",
    "        print('Training Accuracy: {:0.2}'.format(train_accuracy))\n",
    "        print('Test Accuracy: {:0.2}'.format(test_accuracy))\n",
    "\n",
    "        # Confusion matrix labels:\n",
    "        labels = np.array([['true losses','false wins'], ['false losses','true wins']])\n",
    "\n",
    "        # Model evaluation metrics:\n",
    "        confusion_mtrx = confusion_matrix(y_test, predicted)\n",
    "        auc = roc_auc_score(y_test, probabilities)\n",
    "        print('AUC: {:0.2}'.format(auc))\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "        logloss = log_loss(y_test, ensemble_voting.predict_proba(X_test))\n",
    "        print('Logloss: {:0.2}\\n'.format(logloss))\n",
    "    \n",
    "        # Update final counts.\n",
    "        count_train_accuracy+=train_accuracy\n",
    "        count_test_accuracy+=test_accuracy\n",
    "        count_auc+=auc\n",
    "        count_logloss+=logloss\n",
    "    \n",
    "    print('____________________________________')\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('____________________________________')\n",
    "    print('Training Accuracy: {:0.2}'.format(count_train_accuracy/cv))\n",
    "    print('Test Accuracy: {:0.2}'.format(count_test_accuracy/cv))\n",
    "    print('AUC: {:0.2}'.format(count_auc/cv))\n",
    "    print('Logloss: {:0.2}\\n'.format(count_logloss/cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# From hyperparameter tuning section above:\n",
    "# Best Parameters: {'clf__kernel': 'sigmoid', 'clf__C': 100000000.0}\n",
    "# Best Parameters: {'clf__n_estimators': 166, 'clf__max_features': 'sqrt'}\n",
    "# Best Parameters: {'clf__penalty': 'l2', 'clf__C': 10000000.0}\n",
    "# Best Parameters: {'clf__hidden_layer_sizes': (10, 19, 8)}\n",
    "# Best Parameters: {'clf__var_smoothing': 1e-08}\n",
    "# Best Parameters: {'clf__n_estimators': 177, 'clf__learning_rate': 0.7222222222222222}\n",
    "# Best Parameters: {'clf__n_estimators': 188, 'clf__learning_rate': 0.04}\n",
    "# Best Parameters: {'clf__max_depth': 4, 'clf__booster': 'gblinear'}\n",
    "classifiers = [\n",
    "    SVC(probability=True, kernel='sigmoid', C=100000000.0),         \n",
    "    RandomForestClassifier(n_estimators=166, max_features='sqrt'),      \n",
    "    LogisticRegression(penalty='l2', C=10000000.0),          \n",
    "    MLPClassifier(max_iter=500, hidden_layer_sizes=(10, 19, 8)),   \n",
    "    GaussianNB(var_smoothing=1e-08),                  \n",
    "    AdaBoostClassifier(n_estimators=177, learning_rate=0.72),          \n",
    "    GradientBoostingClassifier(n_estimators=188, learning_rate=0.04), \n",
    "    XGBClassifier(max_depth=4, booster='gblinear')             \n",
    "    ]\n",
    "\n",
    "pipeline_classifiers=[]\n",
    "for i in range(0,len(classifiers)):\n",
    "    pipeline_classifiers.append(Pipeline (steps=[('preprocessor', preprocessor),\n",
    "        ('clf'+str(i+1), classifiers[i])]))\n",
    "del classifiers\n",
    "    \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble_voting = Pipeline(steps=[('ensemble', VotingClassifier(estimators=[\n",
    "    ('svc', pipeline_classifiers[0]), \n",
    "    ('rdf' , pipeline_classifiers[1]),\n",
    "    ('lgr' , pipeline_classifiers[2]),\n",
    "    ('mlp', pipeline_classifiers[3]),\n",
    "    ('gau', pipeline_classifiers[4]),\n",
    "    ('ada', pipeline_classifiers[5]), \n",
    "    ('gbt', pipeline_classifiers[6]),\n",
    "    ('xgb', pipeline_classifiers[7])], \n",
    "                                voting='soft', \n",
    "                                # weights = [1,2,3], \n",
    "                                n_jobs=-1))])\n",
    "\n",
    "# import pprint as pp\n",
    "# pp.pprint(sorted(ensemble_voting.get_params().keys())) # used to specify ensemble params, below. \n",
    "clfy_report_ensemble(ensemble_voting, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('svc',  SVC(probability=True, kernel='rbf', C=1.0         )),\n",
    "    ('rdf' , RandomForestClassifier(n_estimators=144, max_features='log2'      )),\n",
    "    ('lgr' , LogisticRegression(penalty='l2', C=10000.0          )),\n",
    "    ('mlp',  MLPClassifier(max_iter=500, hidden_layer_sizes=(5, 18, 8)   )),\n",
    "    ('gau',  GaussianNB(var_smoothing=1e-08)),                  \n",
    "    ('ada',  AdaBoostClassifier(n_estimators=177, learning_rate=0.83          )),\n",
    "    ('gbt',  GradientBoostingClassifier(n_estimators=144, learning_rate=0.04) ),\n",
    "    ('xgb',  XGBClassifier(max_depth=4, booster='gblinear')                      )\n",
    "    ]\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "stack_clf = StackingClassifier(estimators=classifiers, final_estimator=LogisticRegression())\n",
    "pipe_stack_clf = Pipeline(steps=[('preproc', preprocessor), ('stack', stack_clf)])\n",
    "clfy_report_ensemble(pipe_stack_clf, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        stack_clf, np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), \n",
    "        loss='0-1_loss')\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "#         ensemble_voting[0], np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), \n",
    "    VotingClassifier(estimators=[\n",
    "        classifiers[0], \n",
    "        classifiers[1],\n",
    "        classifiers[2],\n",
    "        classifiers[3],\n",
    "        classifiers[4],\n",
    "        classifiers[5], \n",
    "        classifiers[6],\n",
    "        classifiers[7]], \n",
    "                        voting='soft', \n",
    "                        # weights = [1,2,3], \n",
    "                        n_jobs=-1),\n",
    "        np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), \n",
    "        loss='0-1_loss')\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From these results, the both of the VotingClassifier and StackingClassifier models built have:\n",
    "- very low variance  (~0.10 - 0.14)\n",
    "- low-moderate loss (~0.31 - 0.34)\n",
    "- low-moderate bias (~0.30 - 0.32)  </ul>\n",
    "The results above implies that the final model needs to use the StackingClassifier, to lower bias. However, the 10-fold cross-validated accuracy is around the same for the VotingClassifier (Training Accuracy: 0.82, Test Accuracy: 0.69) compared to the StackingClassifier (Training Accuracy: 0.64, Test Accuracy: 0.7). \n",
    "### Therefore, either VotingClassifier or StackingClassifier could be used as the primary prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a bracket using both methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.read_csv('../2019Data/SampleSubmissionStage2.csv')\n",
    "df_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_team1_team2(ID):\n",
    "    \"\"\"Return a tuple with the year, team1 and team2\n",
    "    for each ID in the sample submission file of possible matches.\"\"\"\n",
    "    return (int(x) for x in ID.split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv('../2019Data/df_features.csv')\n",
    "diff = []\n",
    "data = []\n",
    "\n",
    "for i, row in df_predict.iterrows():\n",
    "\n",
    "    year, team1, team2 = get_year_team1_team2(row.ID)\n",
    "\n",
    "    # Save 2018 stats/features for the first ID:\n",
    "    team1 = df_features[(df_features['Season'] == year) & (df_features['TeamID'] == team1)].values[0]\n",
    "\n",
    "    # Save 2018 stats/features for the first ID:\n",
    "    team2 = df_features[(df_features['Season'] == year) & (df_features['TeamID'] == team2)].values[0]\n",
    "\n",
    "    diff = team1 - team2\n",
    "\n",
    "    data.append(diff)\n",
    "\n",
    "n_poss_games = len(df_predict)\n",
    "columns = df_features.columns.get_values()\n",
    "final_predictions = pd.DataFrame(np.array(data).reshape(n_poss_games, np.array(data).shape[1]), columns=(columns))\n",
    "final_predictions.drop(['Season', 'TeamID'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackingClassifier Bracket Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stack_clf.fit(X,y)\n",
    "print(\"StackingClassifier fit on data.\")\n",
    "predictions = pipe_stack_clf.predict_proba(final_predictions)[:, 1]\n",
    "clipped_predictions = np.clip(predictions, 0.05, 0.95)\n",
    "df_predict.Pred = clipped_predictions\n",
    "df_predict.to_csv('search_stk.csv', index=False)\n",
    "\n",
    "from bracketeer import build_bracket\n",
    "b = build_bracket(\n",
    "        outputPath='search_stk.png', # in /Ryan\n",
    "        submissionPath='search_stk.csv',\n",
    "        teamsPath='../2019Data/Stage2DataFiles/Teams.csv',\n",
    "        seedsPath='../2019Data/Stage2DataFiles/NCAATourneySeeds.csv',\n",
    "        slotsPath='../2019Data/Stage2DataFiles/NCAATourneySlots.csv',\n",
    "        year=2019\n",
    ")\n",
    "from IPython.display import Image\n",
    "Image(filename='search_stk.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier Bracket Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_voting.fit(X,y)\n",
    "print(\"VotingClassifier fit on data.\")\n",
    "predictions = ensemble_voting.predict_proba(final_predictions)[:, 1]\n",
    "clipped_predictions = np.clip(predictions, 0.05, 0.95)\n",
    "df_predict.Pred = clipped_predictions\n",
    "df_predict.to_csv('search_voting.csv', index=False)\n",
    "\n",
    "from bracketeer import build_bracket\n",
    "b = build_bracket(\n",
    "        outputPath='search_voting.png', # in /Ryan\n",
    "        submissionPath='search_voting.csv',\n",
    "        teamsPath='../2019Data/Stage2DataFiles/Teams.csv',\n",
    "        seedsPath='../2019Data/Stage2DataFiles/NCAATourneySeeds.csv',\n",
    "        slotsPath='../2019Data/Stage2DataFiles/NCAATourneySlots.csv',\n",
    "        year=2019\n",
    ")\n",
    "from IPython.display import Image\n",
    "Image(filename='search_ens.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
